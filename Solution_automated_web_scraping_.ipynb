{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this solution, I utilized AutoScraper and Selenium to automate the process of web scraping for gathering information about plants. AutoScraper was used to simplify the extraction of structured data from static web pages, while Selenium handled dynamic content by automating browser interactions. This setup allowed me to efficiently collect a wide range of plant-related data, including care instructions, optimal growing conditions, and gardening tips from various websites. The scraped data was then processed for use in a project."
      ],
      "metadata": {
        "id": "FpSP8VahrNMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation and Setup of AutoScraper"
      ],
      "metadata": {
        "id": "A7IxkRX9r3KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YG0xBaa4D7V",
        "outputId": "b7bb68b1-c1e4-40f2-9fd9-3bb4bc5eb69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autoscraper\n",
            "  Downloading autoscraper-1.1.14-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autoscraper) (2.32.3)\n",
            "Collecting bs4 (from autoscraper)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from autoscraper) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->autoscraper) (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autoscraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autoscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autoscraper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autoscraper) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->autoscraper) (2.6)\n",
            "Downloading autoscraper-1.1.14-py3-none-any.whl (10 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, autoscraper\n",
            "Successfully installed autoscraper-1.1.14 bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install autoscraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autoscraper import AutoScraper"
      ],
      "metadata": {
        "id": "b2ypFWEs5RtN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup for Selenium WebDriver with Chromium"
      ],
      "metadata": {
        "id": "J8BKB2q2r_p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzOLhhQ_7JqR",
        "outputId": "a490c679-a5db-4e47-f93e-8fef0c5a8f9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.25.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9alXxgM7k99",
        "outputId": "140b1bd7-7b19-4ebf-e64f-3ce1a62ada53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.83)] [1 InRele\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.83)] [Connecte\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,346 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,308 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,589 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,440 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,153 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,590 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,186 kB]\n",
            "Fetched 21.9 MB in 3s (8,460 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y chromium chromium-browser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmY8PteL7rce",
        "outputId": "b5134cfd-782e-44f3-fb05-99fb37e161e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package chromium is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  chromium-bsu\n",
            "\n",
            "E: Package 'chromium' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGYiLqdq7wGr",
        "outputId": "5d26dfd4-6ae7-40dc-e824-cddf28aafc49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 3s (10.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123807 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124037 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By # Import the By class"
      ],
      "metadata": {
        "id": "8ML_tBu778yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to Initialize Headless Chromium WebDriver"
      ],
      "metadata": {
        "id": "Dva0aw5FseB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument(\"--window-size=1920, 1200\")\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver"
      ],
      "metadata": {
        "id": "tljXcDWf8Cs_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function web_driver() initializes a Selenium WebDriver for Chromium in a \"headless\" mode, which allows the browser to run without a graphical user interface. This is particularly useful for automating web scraping tasks on servers or environments where displaying the browser window is unnecessary.\n",
        "\n",
        "Here’s a breakdown of the options used:\n",
        "\n",
        "\n",
        "\n",
        "*   --verbose: Enables detailed logging, useful for debugging.\n",
        "*  --no-sandbox: Runs the browser in a non-sandboxed mode, often needed for some environments like cloud services.\n",
        "\n",
        "*   --headless: Runs the browser without a UI, which makes the scraping process faster and more resource-efficient.\n",
        "\n",
        "*   --disable-gpu: Disables GPU usage since it's not required for headless operation.\n",
        "\n",
        "*   --window-size=1920, 1200: Sets the browser's window size, useful for ensuring content is properly rendered and visible in the headless mode.\n",
        "*  --disable-dev-shm-usage: Prevents issues related to limited shared memory on some Linux environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "wjbmiOpssqCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver = web_driver()"
      ],
      "metadata": {
        "id": "IYoBYdhV8Hv1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plant_data(url):"
      ],
      "metadata": {
        "id": "x5ILw0PqtrwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:**\n",
        "The function extracts text content from the main part of the webpage and filters out any price-related information."
      ],
      "metadata": {
        "id": "R6VfFQVAtjNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plant_data(url):\n",
        "  driver.get(url)#url will be automatically entered\n",
        "  wanted_list= driver.find_elements(By.XPATH,'//main')\n",
        "  wanted_list=[s.text for s in wanted_list]\n",
        "  f_res=[]\n",
        "  wanted_list=list(set(wanted_list))\n",
        "  #print(wanted_list)\n",
        "  for i in range(len(wanted_list)):\n",
        "    res=wanted_list[i].split('\\n')\n",
        "    for j in range(len(res)):\n",
        "      if res[j].find('$')==-1 and res[j].find('£')==-1:\n",
        "        f_res.append(res[j])\n",
        "  return f_res"
      ],
      "metadata": {
        "id": "fnNTfRPR5Vxg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#all_plants_urls(url, initial_url):"
      ],
      "metadata": {
        "id": "InMy-qUJuHKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:**\n",
        "This function scrapes all links from a given webpage, then filters out specific URLs (such as header or footer links) using the AutoScraper tool. The final result is a list of relevant plant-related URLs."
      ],
      "metadata": {
        "id": "z3-cdbvkuNgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_plants_urls(url,initial_url):\n",
        "  import time\n",
        "  scraper=AutoScraper()\n",
        "  driver.get(url)\n",
        "  time.sleep(5)\n",
        "  all_links=driver.find_elements(By.XPATH,'//main//a')\n",
        "  all_links=[link.get_attribute('href') for link in all_links if link!=None]\n",
        "  scraper=AutoScraper()\n",
        "  ref=[url,initial_url]#example from the footer and example from the header(or the navbar ) , Simply url and initial_url\n",
        "  result=scraper.build(url=url,wanted_list=ref)\n",
        "  list_of_links=list(set(all_links)-set(result))\n",
        "  f_list_of_links=[]\n",
        "  for i in range(len(list_of_links)):\n",
        "    if list_of_links[i] is not None :#Double verification of not existing of None is obligatory\n",
        "      f_list_of_links.append(list_of_links[i])\n",
        "  return f_list_of_links"
      ],
      "metadata": {
        "id": "gkS4E-OOt-oc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Handling and Scraping Logic for Gardening Website"
      ],
      "metadata": {
        "id": "er6ADDqTujPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:**\n",
        "This part of the code takes user input to select between scraping data for a single plant or a collection of plants from a gardening website. It validates the URL structure, handles pagination for multi-page results, and gathers plant information accordingly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O1K0yMkQurJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=input(\"Enter the url of the gardening website:  \")\n",
        "while url.find('https://www.')==-1 and (url.find('plant') or url.find('garden'))==-1:\n",
        "  url=input(\"Enter the url of the gardening website:  \")\n",
        "if len(url.split('/'))>=3:\n",
        "  initial_url='https://'+url.split('/')[2]\n",
        "else:\n",
        "  initial_url=url\n",
        "specification_of_scraping=input(\"Single plant(1) or collection of plants(2):  \")\n",
        "while specification_of_scraping!='1' and specification_of_scraping!='2':\n",
        "  specification_of_scraping=input(\"Single plant(1) or collection of plants(2):\")\n",
        "if specification_of_scraping=='2':\n",
        "  f_all_plants=[]\n",
        "  all_plants=all_plants_urls(url,initial_url)#function to define that extracts all the urls of the plants existing inside the website\n",
        "  print(all_plants)\n",
        "  print(len(all_plants))\n",
        "  for i in range(len(all_plants)):\n",
        "    f_all_plants.append(all_plants[i])\n",
        "  j=1\n",
        "  while len(all_plants)>0:\n",
        "    j=j+1\n",
        "    if url.find('?')!=-1:\n",
        "      new_url=url+'&page='+str(j)\n",
        "    else:\n",
        "      new_url=url+'?page='+str(j)\n",
        "    all_plants=all_plants_urls(new_url,initial_url)\n",
        "    for i in range(len(all_plants)):\n",
        "      f_all_plants.append(all_plants[i])\n",
        "  for j in range(len(f_all_plants)):\n",
        "    data=plant_data(f_all_plants[j])\n",
        "elif specification_of_scraping=='1':\n",
        "  data=plant_data(url)#function to define that extracts the data of the plant url without cleaning it\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "sLJ_TTW-tyK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code handles user input for scraping a gardening website, verifying its structure, and then scraping either data for a single plant or a collection of plants. Here's a breakdown:\n",
        "\n",
        "URL Input and Validation:\n",
        "\n",
        "url = input(\"Enter the url of the gardening website: \"): The user is prompted to input the URL of the gardening website.\n",
        "The while loop ensures the entered URL starts with 'https://www.' and contains keywords like 'plant' or 'garden' (important for ensuring the user enters a relevant URL). If these conditions aren't met, the user is prompted to re-enter the URL.\n",
        "Initial URL Setup:\n",
        "\n",
        "If the entered URL has three or more segments (e.g., https://example.com/plant), it extracts the base URL (https://example.com). Otherwise, it treats the entered URL as the base URL (initial_url).\n",
        "Scraping Mode Selection:\n",
        "\n",
        "specification_of_scraping = input(\"Single plant(1) or collection of plants(2): \"): The user is asked whether they want to scrape a single plant's data (1) or multiple plants (2).\n",
        "The while loop ensures valid input is provided (1 or 2).\n",
        "Collection of Plants (Option 2):\n",
        "\n",
        "If the user selects option 2 (collection of plants):\n",
        "all_plants = all_plants_urls(url, initial_url): Calls the all_plants_urls() function to retrieve all plant-related URLs from the provided gardening website.\n",
        "f_all_plants.append(all_plants[i]): The extracted URLs are stored in a list (f_all_plants).\n",
        "The code handles pagination by appending &page= or ?page= to the URL if there are additional pages, and repeats the process for scraping additional plant URLs.\n",
        "Once all URLs are collected, it loops through them and calls the plant_data() function to scrape and process data for each plant.\n",
        "Single Plant (Option 1):\n",
        "\n",
        "If the user selects option 1 (single plant):\n",
        "data = plant_data(url): Calls the plant_data() function to scrape data from the single plant URL.\n",
        "Data Output:\n",
        "\n",
        "After scraping, it prints the data collected from the URL(s), either for the single plant or the entire collection.\n",
        "\n"
      ],
      "metadata": {
        "id": "M1WiuxntuyFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User Input, URL Validation, and Plant Data Scraping with Pagination"
      ],
      "metadata": {
        "id": "IfWX3H0YvE-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section manages user input, ensures valid gardening website URLs, and facilitates scraping data for either a single plant or multiple plants. It implements pagination to scrape plant data across multiple pages when needed, providing a flexible and robust way to gather data from gardening websites."
      ],
      "metadata": {
        "id": "MWfd_ZGpvKPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "scraper=AutoScraper()\n",
        "driver.get(\"https://www.gardenersworld.com/search?q=plants&tab=plants\")\n",
        "time.sleep(5)\n",
        "all_links=driver.find_elements(By.XPATH,'//main//a')\n",
        "all_links=[link.get_attribute('href') for link in all_links if link!=None]\n",
        "scraper=AutoScraper()\n",
        "ref=[\"https://www.gardenersworld.com\",\"https://www.gardenersworld.com/plants/\"]#example from the footer and example from the header(or the navbar ) , Simply url and initial_url\n",
        "result=scraper.build(url=\"https://www.gardenersworld.com/search?q=plants&tab=plants\",wanted_list=ref)\n",
        "list_of_links=list(set(all_links)-set(result))\n",
        "f_list_of_links=[]\n",
        "for i in range(len(list_of_links)):\n",
        "  if list_of_links[i] is not None :#Double verification of not existing of None is obligatory\n",
        "    f_list_of_links.append(list_of_links[i])\n",
        "print(f_list_of_links)\n",
        "print(len(f_list_of_links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu_VcrOVE-KT",
        "outputId": "46e33ebe-d24d-47ff-875a-1e6191cc77f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.gardenersworld.com/plants/actinidia-kolomikta/', 'https://www.gardenersworld.com/plants/gunnera-tinctoria/', 'https://www.gardenersworld.com/plants/cobaea-scandens/', 'https://www.gardenersworld.com/plants/impatiens-new-guinea-group/', 'https://www.gardenersworld.com/plants/callistemon-citrinus-splendens/', 'https://www.gardenersworld.com/plants/phyllostachys-nigra/', 'https://www.gardenersworld.com/plants/buddleja-globosa/', 'https://www.gardenersworld.com/plants/gaultheria-procumbens/', 'https://www.gardenersworld.com/plants/lysimachia-vulgaris/', 'https://www.gardenersworld.com/plants/monarda-didyma/', 'https://www.gardenersworld.com/plants/anemone-blanda/', 'https://www.gardenersworld.com/how-to/grow-plants/peperomia-argyreia/', 'https://www.gardenersworld.com/how-to/grow-plants/clusia-rosea/', 'https://www.gardenersworld.com/plants/mahonia-japonica/', 'https://www.gardenersworld.com/how-to/grow-plants/ulex-europaeus/', 'https://www.gardenersworld.com/how-to/grow-plants/sarracenia-juthatip-soper/', 'https://www.gardenersworld.com/plants/leptinella-squalida-platts-black/', 'https://www.gardenersworld.com/plants/santolina-chamaecyparissus/', 'https://www.gardenersworld.com/plants/alocasia-black-velvet/', 'https://www.gardenersworld.com/plants/dracaena-fragrans/', 'https://www.gardenersworld.com/how-to/grow-plants/coleus-caninus/', 'https://www.gardenersworld.com/how-to/grow-plants/epipremnum-aureum-neon/', 'https://www.gardenersworld.com/plants/schefflera-arboricola/', 'https://www.gardenersworld.com/plants/ammi-visnaga/', 'https://www.gardenersworld.com/plants/nicotiana-sylvestris-2/', 'https://www.gardenersworld.com/plants/alocasia-zebrina/', 'https://www.gardenersworld.com/plants/alocasia-wentii/', 'https://www.gardenersworld.com/how-to/grow-plants/pennisetum-alopecuroides/', 'https://www.gardenersworld.com/plants/tradescantia-zebrina/', 'https://www.gardenersworld.com/plants/dianthus-pink-kisses/']\n",
            "30\n"
          ]
        }
      ]
    }
  ]
}